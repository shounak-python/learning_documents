{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8df0e473",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'airflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Airflow using Bash Script\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Task 1.0: Import Libraries\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m timedelta\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mairflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DAG\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mairflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdates\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m days_ago\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mairflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moperators\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbash_operator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BashOperator\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'airflow'"
     ]
    }
   ],
   "source": [
    "# Airflow using Bash Script\n",
    "\n",
    "# Task 1.0: Import Libraries\n",
    "from datetime import timedelta\n",
    "from airflow import DAG\n",
    "from airflow.utils.dates import days_ago\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "\n",
    "# Task 1.1: DAG Arguments\n",
    "default_args = {\n",
    "    \"owner\": \"Shounak\",\n",
    "    \"start_date\": days_ago(0),\n",
    "    \"emails\": [\"shounak@mail.com\"],\n",
    "    \"email_on_failure\": False,\n",
    "    \"email_on_retry\": False,\n",
    "    \"retries\": 1,\n",
    "    \"retry_delay\": timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "# Task 1.2: DAG Definition\n",
    "dag = DAG(\n",
    "    dag_id=\"ETL_toll_data\",\n",
    "    default_args=default_args,\n",
    "    description=\"Apache Airflow Final Assignment\",\n",
    "    schedule=timedelta(days=1),\n",
    ")\n",
    "\n",
    "# Task 1.3: Unzip Data\n",
    "unzip_data = BashOperator(\n",
    "    task_id=\"unzip_data\",\n",
    "    bash_command=\"sudo tar zxvf /home/project/airflow/dags/finalassignment/tolldata.tgz\",\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Task 1.4: Extract data from .csv\n",
    "extract_data_from_csv = BashOperator(\n",
    "    task_id=\"extract_data_from_csv\",\n",
    "    bash_command='sudo cut -d\",\" -f1-4 /home/project/airflow/dags/finalassignment/vehicle-data.csv > '\\\n",
    "    '/home/project/airflow/dags/finalassignment/staging/csv_data.csv',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Task 1.5: Extract data from .tsv\n",
    "extract_data_from_tsv = BashOperator(\n",
    "    task_id=\"extract_data_from_tsv\",\n",
    "    bash_command=\"sudo cut -f5-7 /home/project/airflow/dags/finalassignment/tollplaza-data.tsv > \"\\\n",
    "    \"/home/project/airflow/dags/finalassignment/staging/tsv_data.csv\",\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Task 1.6: Extract data from fixed width file\n",
    "extract_data_from_fixed_width = BashOperator(\n",
    "    task_id=\"extract_data_from_fixed_width\",\n",
    "    bash_command=\"cut -c59-62,63-67 /home/project/airflow/dags/finalassignment/payment-data.txt > \"\\\n",
    "    \"/home/project/airflow/dags/finalassignment/staging/fixed_width_data.csv\",\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Task 1.7: Consolidate data\n",
    "consolidate_data = BashOperator(\n",
    "    task_id=\"consolidate_data\",\n",
    "    bash_command=\"paste /home/project/airflow/dags/finalassignment/staging/csv_data.csv \"\\\n",
    "    \"/home/project/airflow/dags/finalassignment/staging/tsv_data.csv \"\\\n",
    "    \"/home/project/airflow/dags/finalassignment/staging/fixed_width_data.csv > \"\\\n",
    "    \"/home/project/airflow/dags/finalassignment/staging/extracted_data.csv\",\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Task 1.8: Transform data\n",
    "transform_data = BashOperator(\n",
    "    task_id=\"transform_data\",\n",
    "    bash_command='tr \"[:lower:]\" \"[:upper:]\" < /home/project/airflow/dags/finalassignment/staging/extracted_data.csv > '\\\n",
    "    '/home/project/airflow/dags/finalassignment/staging/transformed_data.csv',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Task 1.9: Task Pipeline\n",
    "unzip_data >> extract_data_from_csv >> extract_data_from_tsv >> extract_data_from_fixed_width >> consolidate_data >> transform_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dab6038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Airflow using Bash Script\n",
    "\n",
    "# Task 1.0: Import Libraries\n",
    "from datetime import timedelta\n",
    "from airflow import DAG\n",
    "from airflow.utils.dates import days_ago\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "\n",
    "# Task 1.1: DAG Arguments\n",
    "default_args = {\n",
    "    \"owner\": \"Shounak\",\n",
    "    \"start_date\": days_ago(0),\n",
    "    \"emails\": [\"shounak@mail.com\"],\n",
    "    \"email_on_failure\": False,\n",
    "    \"email_on_retry\": False,\n",
    "    \"retries\": 1,\n",
    "    \"retry_delay\": timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "# Task 1.2: DAG Definition\n",
    "dag = DAG(\n",
    "    dag_id=\"ETL_toll_data\",\n",
    "    default_args=default_args,\n",
    "    description=\"Apache Airflow Final Assignment\",\n",
    "    schedule=timedelta(days=1),\n",
    ")\n",
    "\n",
    "# Task 1.3: Unzip Data\n",
    "unzip_data = BashOperator(\n",
    "    task_id=\"unzip_data\",\n",
    "    bash_command=\"sudo tar zxvf /home/project/airflow/dags/finalassignment/tolldata.tgz\",\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Task 1.4: Extract data from .csv\n",
    "extract_data_from_csv = BashOperator(\n",
    "    task_id=\"extract_data_from_csv\",\n",
    "    bash_command='sudo cut -d\",\" -f1-4 /home/project/airflow/dags/finalassignment/vehicle-data.csv > /home/project/airflow/dags/finalassignment/staging/csv_data.csv',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Task 1.5: Extract data from .tsv\n",
    "extract_data_from_tsv = BashOperator(\n",
    "    task_id=\"extract_data_from_tsv\",\n",
    "    bash_command=\"sudo cut -f5-7 /home/project/airflow/dags/finalassignment/tollplaza-data.tsv > /home/project/airflow/dags/finalassignment/staging/tsv_data.csv\",\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Task 1.6: Extract data from fixed width file\n",
    "extract_data_from_fixed_width = BashOperator(\n",
    "    task_id=\"extract_data_from_fixed_width\",\n",
    "    bash_command=\"cut -c59-62,63-67 /home/project/airflow/dags/finalassignment/payment-data.txt > /home/project/airflow/dags/finalassignment/staging/fixed_width_data.csv\",\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Task 1.7: Consolidate data\n",
    "consolidate_data = BashOperator(\n",
    "    task_id=\"consolidate_data\",\n",
    "    bash_command=\"paste /home/project/airflow/dags/finalassignment/staging/csv_data.csv /home/project/airflow/dags/finalassignment/staging/tsv_data.csv /home/project/airflow/dags/finalassignment/staging/fixed_width_data.csv > /home/project/airflow/dags/finalassignment/staging/extracted_data.csv\",\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Task 1.8: Transform data\n",
    "transform_data = BashOperator(\n",
    "    task_id=\"transform_data\",\n",
    "    bash_command='tr \"[:lower:]\" \"[:upper:]\" < /home/project/airflow/dags/finalassignment/staging/extracted_data.csv > /home/project/airflow/dags/finalassignment/staging/transformed_data.csv',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Task 1.9: Task Pipeline\n",
    "unzip_data >> extract_data_from_csv >> extract_data_from_tsv >> extract_data_from_fixed_width >> consolidate_data >> transform_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1794d61",
   "metadata": {},
   "source": [
    "# Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88876c55",
   "metadata": {},
   "source": [
    "1. Set up kafka:\n",
    "`wget https://archive.apache.org/dist/kafka/2.8.0/kafka_2.12-2.8.0.tgz`\n",
    "`tar -xzf kafka_2.12-2.8.0.tgz`\n",
    "\n",
    "2. Start mysql and create database tolldata and table in it with name livetolldata:\n",
    "`start_mysql`\n",
    "`mysql --host=127.0.0.1 --port=3306 --user=root --password=MjE2Njctc2hvdW5h`\n",
    "`create database tolldata;`\n",
    "`use tolldata;`\n",
    "`create table livetolldata(timestamp datetime,vehicle_id int,vehicle_type char(15),toll_plaza_id smallint);`\n",
    "`bye`\n",
    "\n",
    "3. Install python module for kafka and mysql\n",
    "`pip3 install kafka-python`\n",
    "`pip3 install mysql-connector-python `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3a01a6",
   "metadata": {},
   "source": [
    "1. Start Zookeeper:\n",
    "`cd kafka_2.12-2.8.0\n",
    "bin/zookeeper-server-start.sh config/zookeeper.properties`\n",
    "\n",
    "2. Starts Kafka:\n",
    "`cd kafka_2.12-2.8.0\n",
    "bin/kafka-server-start.sh config/server.properties`\n",
    "\n",
    "3. Create Topic:\n",
    "`cd kafka_2.12-2.8.0\n",
    "bin/kafka-topics.sh --create --topic toll --bootstrap-server localhost:9092 `\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7558d56",
   "metadata": {},
   "source": [
    "Download Notebook using wget\n",
    "`https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0250EN-SkillsNetwork/labs/Final%20Assignment/toll_traffic_generator.py`\n",
    "\n",
    "\n",
    "Code in the file:\n",
    "`    \n",
    "\"\"\"\n",
    "Top Traffic Simulator\n",
    "\"\"\"\n",
    "from time import sleep, time, ctime\n",
    "from random import random, randint, choice\n",
    "from kafka import KafkaProducer\n",
    "producer = KafkaProducer(bootstrap_servers='localhost:9092')\n",
    "\n",
    "TOPIC = 'set your topic here'\n",
    "\n",
    "VEHICLE_TYPES = (\"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\", \"car\",\n",
    "                 \"car\", \"car\", \"car\", \"truck\", \"truck\", \"truck\",\n",
    "                 \"truck\", \"van\", \"van\")\n",
    "for _ in range(100000):\n",
    "    vehicle_id = randint(10000, 10000000)\n",
    "    vehicle_type = choice(VEHICLE_TYPES)\n",
    "    now = ctime(time())\n",
    "    plaza_id = randint(4000, 4010)\n",
    "    message = f\"{now},{vehicle_id},{vehicle_type},{plaza_id}\"\n",
    "    message = bytearray(message.encode(\"utf-8\"))\n",
    "    print(f\"A {vehicle_type} has passed by the toll plaza {plaza_id} at {now}.\")\n",
    "    producer.send(TOPIC, message)\n",
    "    sleep(random() * 2)\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368faaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Exercise 2 - Start Kafka\n",
    "Task 2.1 - Start Zookeeper\n",
    "Start zookeeper server.\n",
    "\n",
    "Take a screenshot of the command you run.\n",
    "\n",
    "Name the screenshot start_zookeeper.jpg. (Images can be saved with either the .jpg or .png extension.)\n",
    "\n",
    "Task 2.2 - Start Kafka server\n",
    "Start Kafka server\n",
    "\n",
    "Take a screenshot of the command you run.\n",
    "\n",
    "Name the screenshot start_kafka.jpg. (Images can be saved with either the .jpg or .png extension.)\n",
    "\n",
    "Task 2.3 - Create a topic named toll\n",
    "Create a Kakfa topic named toll\n",
    "\n",
    "Take a screenshot of the command you run.\n",
    "\n",
    "Name the screenshot create_toll_topic.jpg. (Images can be saved with either the .jpg or .png extension.)\n",
    "\n",
    "Task 2.4 - Download the Toll Traffic Simulator\n",
    "Download the toll_traffic_generator.py from the url given below using 'wget'.\n",
    "\n",
    "https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0250EN-SkillsNetwork/labs/Final%20Assignment/toll_traffic_generator.py\n",
    "\n",
    "Open the code using the theia editor using the \"Menu --> File -->Open\" option.\n",
    "\n",
    "Take a screenshot of the task code.\n",
    "\n",
    "Name the screenshot download_simulator.jpg. (Images can be saved with either the .jpg or .png extension.)\n",
    "\n",
    "Task 2.5 - Configure the Toll Traffic Simulator\n",
    "Open the toll_traffic_generator.py and set the topic to toll.\n",
    "\n",
    "Take a screenshot of the task code with the topic clearly visible.\n",
    "\n",
    "Name the screenshot configure_simulator.jpg. (Images can be saved with either the .jpg or .png extension.)\n",
    "\n",
    "Task 2.6 - Run the Toll Traffic Simulator\n",
    "Run the toll_traffic_generator.py.\n",
    "\n",
    "Hint : python3 <pythonfilename> runs a python program on the theia lab.\n",
    "\n",
    "Take a screenshot of the output of the simulator.\n",
    "\n",
    "Name the screenshot simulator_output.jpg. (Images can be saved with either the .jpg or .png extension.)\n",
    "\n",
    "Task 2.7 - Configure streaming_data_reader.py\n",
    "Download the streaming_data_reader.py from the url below using 'wget'.\n",
    "\n",
    "https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0250EN-SkillsNetwork/labs/Final%20Assignment/streaming_data_reader.py\n",
    "\n",
    "Open the streaming_data_reader.py and modify the following details so that the program can connect to your mysql server.\n",
    "\n",
    "TOPIC\n",
    "\n",
    "DATABASE\n",
    "\n",
    "USERNAME\n",
    "\n",
    "PASSWORD\n",
    "\n",
    "Take a screenshot of the code you modified.\n",
    "\n",
    "Name the screenshot streaming_reader_code.jpg. (Images can be saved with either the .jpg or .png extension.)\n",
    "\n",
    "Task 2.8 - Run streaming_data_reader.py\n",
    "Run the streaming_data_reader.py\n",
    "\n",
    "Take a screenshot of the output of the streamingdatareader.py.\n",
    "\n",
    "Name the screenshot data_reader_output.jpg. (Images can be saved with either the .jpg or .png extension.)\n",
    "\n",
    "python3 streaming_data_reader.py\n",
    "Task 2.9 - Health check of the streaming data pipeline.\n",
    "If you have done all the steps till here correctly, the streaming toll data would get stored in the table livetolldata.\n",
    "\n",
    "List the top 10 rows in the table livetolldata.\n",
    "\n",
    "Take a screenshot of the command and the output.\n",
    "\n",
    "Name the screenshot output_rows.jpg. (Images can be saved with either the .jpg or .png extension.)\n",
    "\n",
    "This concludes the assignment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
