{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aded79f0",
   "metadata": {},
   "source": [
    "# A. Intro\n",
    "<br>1. Start Apache Airflow: `start_airflow`\n",
    "<br> **°>>** username: `airflow`\n",
    "<br> **°>>** password: `MTE1OTktc2hvdW5h`\n",
    "<br> **°>>** UI_url: https://shounakdeshp-8080.theiadocker-3-labs-prod-theiak8s-4-tor01.proxy.cognitiveclass.ai\n",
    "<br><br>2. List of DAGs: `airflow dags list`\n",
    "<br><br>3. List of Tasks in a DAG of name DAG_NAME: `airflow tasks list DAG_NAME`\n",
    "<br><br>4. Pause a DAG of name DAG_NAME: `airflow dags pause DAG_NAME`\n",
    "<br><br>5. Un-pause a DAG of name DAG_NAME: `airflow dags unpause DAG_NAME`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa41dfbb",
   "metadata": {},
   "source": [
    "B. Components of DAG\n",
    "\n",
    "1. Import libraries\n",
    "2. DAG Arguments\n",
    "3. DAG Definition\n",
    "4. Task Definition\n",
    "5. Task Pipelines\n",
    "\n",
    "---\n",
    "\n",
    "1. Import libraries:\n",
    "<br><br>Import datetime for time related activities<br>`from datetime import timedelta`\n",
    "<br><br>The DAG object; we'll need this to instantiate a DAG<br>`from airflow import DAG`\n",
    "<br><br>Operators; we need this to write tasks!<br>`from airflow.operators.bash_operator import BashOperator`\n",
    "<br><br>This makes scheduling easy<br>`from airflow.utils.dates import days_ago`\n",
    "\n",
    "---\n",
    "\n",
    "2. DAG Arguments:<br>\n",
    "`default_args = {\n",
    "    'owner': 'Ramesh Sannareddy',\n",
    "    'start_date': days_ago(0),\n",
    "    'email': ['ramesh@somemail.com'],\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': True,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}`\n",
    "\n",
    "---\n",
    "\n",
    "3. DAG Definition:<br>\n",
    "`dag = DAG(\n",
    "    dag_id='sample-etl-dag',\n",
    "    default_args=default_args,\n",
    "    description='Sample ETL DAG using Bash',\n",
    "    schedule_interval=timedelta(days=1),\n",
    ")`\n",
    "\n",
    "---\n",
    "\n",
    "4. Task Definition:<br>\n",
    "\n",
    "First Task 'extract':<br><br>\n",
    "`\n",
    "extract = BashOperator(\n",
    "        task_id='extract',\n",
    "        bash_command='echo \"extract\"',\n",
    "        dag=dag,\n",
    ")\n",
    "`\n",
    "\n",
    "<br><br>Second Task 'transform':<br><br>\n",
    "`\n",
    "transform = BashOperator(\n",
    "        task_id='transform',\n",
    "        bash_command='echo \"transform\"',\n",
    "        dag=dag\n",
    ")\n",
    "`\n",
    "\n",
    "<br><br>Third Task 'load':<br><br>\n",
    "`\n",
    "load = BashOperator(\n",
    "        task_id='load',\n",
    "        bash_command='echo \"load\"',\n",
    "        dag=dag\n",
    ")\n",
    "`\n",
    "\n",
    "---\n",
    "\n",
    "5. Task Pipelines\n",
    "\n",
    "Example:\n",
    "`exract >> transform >> load`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20d3f48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cad9d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from datetime import timedelta\\nfrom airflow import DAG\\nfrom airflow.operators.bash_operator import BashOperator\\nfrom airflow.utils.dates import days_ago\\n\\n\\ndefault_args = {\\n    \\'owner\\': \\'Shounak\\',\\n    \\'start_date\\' : days_ago(0),\\n    \\'email\\' : [\\'emal@email.com\\'],\\n    \\'email_on_failure\\': False,\\n    \\'email_on_retry\\':False,\\n    \\'retries\\': 1,\\n    \\'retry_delay\\':timedelta(minutes=5)\\n}\\n\\ndag = DAG(\\n    \\'ETL_Server_Access_Log_Processing\\',\\n    default_args=default_args,\\n    description=\"etl_server_access_log_process process\",\\n    schedule_interval=timedelta(days=1)\\n)\\n\\ndownload = BashOperator(\\n    task_id=\"download\",\\n    bash_command=\\'wget \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0250EN-SkillsNetwork/labs/Apache%20Airflow/Build%20a%20DAG%20using%20Airflow/web-server-access-log.txt\"\\',\\n    dag=dag\\n)\\n\\nextract = BashOperator(\\n    task_id=\\'extract\\',\\n    bash_command=\\'cut -d\"#\" -f1,4 web-server-access-log.txt > /home/project/airflow/dags/extracted.txt\\',\\n    dag=dag\\n)\\n\\ntransform = BashOperator(\\n    task_id=\\'transform\\',\\n    bash_command=\\'tr \"[:lower:]\" \"[:upper:]\" < /home/project/airflow/dags/extracted.txt > /home/project/airflow/dags/capitalized.txt\\',\\n    dag=dag\\n)\\n\\nload = BashOperator(\\n    task_id=\\'load\\',\\n    bash_command=\\'zip log.zip capitalized.txt\\',\\n    dag=dag\\n)\\n\\n\\ndownload >> extract >> transform >> load\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###############################PythonFileForDAG###############################\n",
    "\"\"\"from datetime import timedelta\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from airflow.utils.dates import days_ago\n",
    "\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'Shounak',\n",
    "    'start_date' : days_ago(0),\n",
    "    'email' : ['emal@email.com'],\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry':False,\n",
    "    'retries': 1,\n",
    "    'retry_delay':timedelta(minutes=5)\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    'ETL_Server_Access_Log_Processing',\n",
    "    default_args=default_args,\n",
    "    description=\"etl_server_access_log_process process\",\n",
    "    schedule_interval=timedelta(days=1)\n",
    ")\n",
    "\n",
    "download = BashOperator(\n",
    "    task_id=\"download\",\n",
    "    bash_command='wget \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0250EN-SkillsNetwork/labs/Apache%20Airflow/Build%20a%20DAG%20using%20Airflow/web-server-access-log.txt\"',\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "extract = BashOperator(\n",
    "    task_id='extract',\n",
    "    bash_command='cut -d\"#\" -f1,4 web-server-access-log.txt > /home/project/airflow/dags/extracted.txt',\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "transform = BashOperator(\n",
    "    task_id='transform',\n",
    "    bash_command='tr \"[:lower:]\" \"[:upper:]\" < /home/project/airflow/dags/extracted.txt > /home/project/airflow/dags/capitalized.txt',\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "load = BashOperator(\n",
    "    task_id='load',\n",
    "    bash_command='zip log.zip capitalized.txt',\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "\n",
    "download >> extract >> transform >> load\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e83e40",
   "metadata": {},
   "source": [
    "### Submit DAG\n",
    "`sudo cp ETL_toll_data.py $AIRFLOW_HOME/dags/`\n",
    "<br><br>See DAGs list:<br>`airflow dags list`\n",
    "<br><br>If there is error, DAG will not appear. See error using:<br>`airflow dags list-import-errors`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
